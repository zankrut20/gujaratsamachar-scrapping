{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pBPoRgaqyBXuyhEjTCcsx7DuZAjyIlrC",
      "authorship_tag": "ABX9TyN1BUGnzrOTYXkx41rpqDXW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zankrut20/gujaratsamachar-scrapping/blob/master/newspaper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pypdf2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcOvBstmHSnO",
        "outputId": "a0f0b1be-9ff3-415f-f667-81c6874969f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from pypdf2) (4.4.0)\n",
            "Installing collected packages: pypdf2\n",
            "Successfully installed pypdf2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "import os\n",
        "import datetime\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from PyPDF2 import  PdfReader, PdfMerger\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/Newspaper')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vx8VhA6DCeM",
        "outputId": "384bd503-b7a9-49af-9743-6657b1d9bbb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Folders\n",
        "dir_names = ['images/', 'PDFs/']\n",
        "for name in dir_names:\n",
        "    try:\n",
        "        os.mkdir(name)\n",
        "    except:\n",
        "        FileExistsError\n",
        "        print(\"Foler already exists\", name)\n",
        "\n",
        "# Scraping Page Links\n",
        "html = \"https://epaper.gujaratsamachar.com/ahmedabad/\" + datetime.datetime.today().strftime('%d-%m-%Y') + \"/1\"\n",
        "r = requests.get(html)\n",
        "print(\"Website is connected\")\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "pages = soup.find(\"ul\", {\"class\": \"nav nav-tabs nav-dots border-bottom-0\"})\n",
        "listofPage = []\n",
        "for page in pages.find_all(\"a\", {\"class\":\"anchor_click\"}, href = True):\n",
        "    listofPage.append(page['href'])\n",
        "\n",
        "# Downloading Images\n",
        "def download_image(url, file_path, file_name):\n",
        "    full_path = file_path + file_name + '.jpg'\n",
        "    urllib.request.urlretrieve(url, full_path)\n",
        "\n",
        "# Scraping Image Links\n",
        "img = []\n",
        "for pages in listofPage:\n",
        "    p = BeautifulSoup(requests.get(pages).content, 'html.parser')\n",
        "    s = p.find(\"img\", {\"class\":\"w-100 sky epaper_page\"})\n",
        "    img.append(s.get('src'))\n",
        "print(\"Page links were scrapped!\")\n",
        "\n",
        "# Downloading Images\n",
        "for i, url in enumerate(img):\n",
        "    # construct the file name for the current image\n",
        "    file_name = f\"{i+1:02d}\"\n",
        "    # download the image using the current file name\n",
        "    download_image(url, file_path=\"images/\", file_name=file_name)\n",
        "print(\"All pages downloaded as images.\")\n",
        "\n",
        "# Converting Images to PDF\n",
        "image_list = os.listdir(\"images/\") # list of images in images folder\n",
        "for image in image_list:\n",
        "    fimg = Image.open(\"images/\" + image)\n",
        "    a4img = Image.new(\"RGB\", (2800, 3974), (255, 255, 255))\n",
        "    a4img.paste(fimg, fimg.getbbox())\n",
        "    a4img.save(\"PDFs/\" + image + \".pdf\", \"PDF\", quality = 70)\n",
        "print(\"All images are converted to PDF format.\")\n",
        "\n",
        "# Merging PDFs\n",
        "pdfs = os.listdir(\"PDFs/\")\n",
        "pdfs.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
        "merger = PdfMerger()\n",
        "for file in pdfs:\n",
        "    merger.append(PdfReader(\"PDFs/\" + file, 'rb'))\n",
        "merger.write(\"Gujarat Samachar_\" + datetime.datetime.today().strftime('%d-%m-%Y') + \".pdf\")\n",
        "print(\"All PDFs are merged into one PDF file.\")\n",
        "\n",
        "# Deleting all Images\n",
        "for image in image_list:\n",
        "    os.remove(\"images/\" + image)\n",
        "print(\"Deleted All Images.\")\n",
        "# Deleting all Single PDFs\n",
        "for pdf in pdfs:\n",
        "    os.remove(\"PDFs/\" + pdf)\n",
        "print(\"Deleted All Single PDFs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4LjWy4IGofs",
        "outputId": "a7130ddf-f291-49a4-c585-f0ffe1dc410d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Foler already exists images/\n",
            "Foler already exists PDFs/\n",
            "Website is connected\n",
            "Page links were scrapped!\n",
            "All pages downloaded as images.\n",
            "All images are converted to PDF format.\n",
            "All PDFs are merged into one PDF file.\n",
            "Deleted All Images.\n",
            "Deleted All Single PDFs\n"
          ]
        }
      ]
    }
  ]
}